{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Neural Networks\n",
    "---\n",
    "# 03 - Multilayer Perceptron\n",
    "\n",
    "Vamos esquentar as coisas!\n",
    "Nesse tutorial, iremos estudar a evolução da Singlelayer Perceptron: a **Multilayer Perceptron** (SLP). Essa é uma das arquiteturas de Rede Neural mais populares até hoje, e é de utilização genérica (pode servir para diversas aplicações).\n",
    "\n",
    "Diferentemente da SLP, que só funciona bem para problemas que são linearmente separáveis, essa Rede Neural é capaz de realizar classificações mais complexas, mesmo que as _decisions boundaries_ tenham formatos menos \"comportados\". Essa característica da MLP advém do fato de que a arquitetura dessa rede possui uma camada extra entre a _Camada de Entrada_ e a _Camada de Saída_, denominada _Camada Oculta_, na qual cada neurônio acumula (como seu Potencial de Rede), uma combinação linear das ativações da camada anterior. Como as ativações são não-lineares (devido à Sigmoide), o Potencial de Rede na camada oculta são transformaçoes não-lineares dos atributos de uma observação. Abaixo, uma illustração da Multilayer Perceptron.\n",
    "\n",
    "<img src=\"slides/imgs/mlp_01.png\" alt=\"singlelayer perceptron\" width=\"500px\"/>\n",
    "\n",
    "Neste notebook, iremos modificar algumas da funções auxiliares implementadas pelo Singlelayer Perceptron para incluir a ação da Camada Oculta. Além disso, iremos desenvolver o algoritmo especial de treinamento de Redes Neurais multicamadas: o **Backpropagation**. Após isso, exibiremos o comportamento dessa rede a um comum problema de _Classificação Não-Linear_, que não pode ser resolvido por Redes Neurais simples como o SLP.\n",
    "\n",
    "### Sumário\n",
    "* [Parte 1 - Funções Básicas de Redes Neurais Multicamadas](#Parte-1---Funções-Básicas-de-Redes-Neurais-Multicamada)\n",
    "* [Parte 2 - Treinando a Rede Neural Multicamada](#Parte-2---Treinando-a-Rede-Neural-Multicamada)\n",
    "* [Parte 3 - Classificação Binária com MLP](#Parte-3---Classificação-Binária-com-MLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas, Variáveis Importantes e Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## BIBLIOTECAS ##\n",
    "import pandas as pd                 # <-- Representação e Manipulação dos Dados\n",
    "import numpy as np                  # <-- Biblioteca para Operações Matriciais e Complexas\n",
    "import matplotlib.pyplot as plt     # <-- Biblioteca para Visualização de Dados\n",
    "\n",
    "## VARIÁVEIS IMPORTANTES ##\n",
    "colorpad = [\"#d11141\", \"#00aedb\", \"#00b159\", \"#f37735\", \"#ffc425\"]\n",
    "colorpadBG = [\"#00aedb50\", \"#d1114150\", \"#00b15950\", \"#f3773550\", \"#ffc42550\"]\n",
    "\n",
    "## FUNÇÕES AUXILIARES ##\n",
    "# Plota um fundo colorido para as visualizações\n",
    "def plotContour(X, model, theta, nClasses=2):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()], theta, False)[-1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contour(xx, yy, Z, colors=\"#2A2A2A\", linewidths=2)\n",
    "    plt.contourf(xx, yy, Z, levels=range(-1,nClasses), colors=colorpadBG)\n",
    "    \n",
    "# Definição da Função Sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Definição da Função de Cálculo de Acurácia\n",
    "def accuracyFunction(X, y, W):\n",
    "    Y_pred = forward(X, W, False)[-1]\n",
    "    return 100 * (1 - (1 / np.size(y)) * np.sum((Y_pred - y) ** 2))\n",
    "    \n",
    "# Essa linha abaixo é apenas para que os \n",
    "# plots sejam gerados na mesma célula do código\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Neste tutorial, utilizaremos um dataset artificial, criado apenas para servir como base à explicação dos conceitos relacionados ao treino de Redes Neurais de multicamadas. \n",
    "\n",
    "**Obs.:** Note como os dados não podem ser separados por uma linha reta, como era o caso dos dados que utilizamos no tutorial do SLP.  \n",
    "**Obs.2:** O código abaixo já está pronto. Para mais detalhes sobre como resgatar e manipular dados de arquivos, refira-se ao Tutorial 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CARREGANDO E PRÉ-PROCESSANDO OS DADOS ##\n",
    "# Carregando os dados a partir do arquivo .csv\n",
    "data = pd.read_csv(\"../datasets/data_02.csv\")\n",
    "\n",
    "# Armazenando as dimensões dos dados\n",
    "m = data.shape[0]\n",
    "n = data.shape[1]-1\n",
    "\n",
    "# Transformando as Classes em Números\n",
    "data[\"Class\"] = data[\"Class\"].astype('category').cat.codes\n",
    "\n",
    "# Separação do Conjunto de Treino e Conjunto de Teste\n",
    "np.random.seed(2)\n",
    "trainingSize = int(0.8 * m)\n",
    "indexes = np.random.randint(0, m, m)\n",
    "\n",
    "trainData = data.iloc[indexes[:trainingSize]]\n",
    "testData = data.iloc[indexes[trainingSize:]]\n",
    "\n",
    "# Obtendo matrizes (formato Numpy) correspondentes\n",
    "X_train = trainData.iloc[:,:-1].values\n",
    "y_train = trainData.iloc[:, -1].values\n",
    "\n",
    "X_test = testData.iloc[:,:-1].values\n",
    "y_test = testData.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## VISUALIZAÇÃO CONDICIONADA COM O MATPLOTLIB E NUMPY ##\n",
    "plt.figure()\n",
    "\n",
    "a_class = np.where(y_train == 1)\n",
    "b_class = np.where(y_train == 0)\n",
    "\n",
    "plt.title(\"Artificial Dataset\"); plt.xlabel(\"$X_1$\"); plt.ylabel(\"$X_2$\")\n",
    "\n",
    "plt.scatter(X_train[a_class,0], X_train[a_class,1], marker=\"s\", color=colorpad[0], edgecolor=\"#2A2A2A\", label=\"Class A\")\n",
    "plt.scatter(X_train[b_class,0], X_train[b_class,1], marker=\"o\", color=colorpad[1], edgecolor=\"#2A2A2A\", label=\"Class B\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 1 - Funções Básicas de Redes Neurais Multicamada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As Redes Neurais multicamadas possuem camadas intermediárias (chamadas de _Camadas Ocultas_) que são resultados de um processo de ativação dos neurônios das camadas anteriores. Por esse motivo, a Rede Neural é representada por uma **tensor de pesos** (matriz com 3 ou mais dimensões), onde cada elemento é uma matrix representando as transições entre duas camadas, e cada coluna dessas matrizes representam os pesos das sinapses de uma camada para um neurônio. Assim como anteriormente, essa matriz também precisa ser calculada automaticamente pelo processo de aprendizagem.\n",
    "\n",
    "Para isso, implementaremos as duas principais funções de uma Rede Neural multicamada:\n",
    "\n",
    "* **Forward Propagation (Propagação Direta):** essa função é responsável por realizar a ativação de cada camada da Rede Neural, desde a Camada de Entrada até a Camada de Saída, e retornar o valor armazenado em cada um desses Neurônios. Perceba que o que é o \"passado adiante\" aqui são os sinais de entrada (atributos) de uma observação recebida pela Rede, até obtermos a probabilidade da observação pertencer a cada classe.\n",
    "\n",
    "* **Backpropagation (Propagação Inversa):** esse é o [algoritmo](https://en.wikipedia.org/wiki/Backpropagation#Pseudocode) mais popular para calcular os gradientes dos pesos de uma Rede Neural multicamada. Foi utilizado em Redes Neurais primeiramente em 1986, que foi um evento marcante na história dessa ciência, e até hoje é extremamente popular. Esse algoritmo consiste em calcular, da última até a primeira camada, o erro acumulado em cada Neurônio. Isso é feito ao calcular a diferença entre a classe prevista e a classe real (o erro da Camada de Saída), e então \"passar para trás\" o valor desse erro ponderado pelos pesos de cada ativação. Esses erros podem ser diretamente utilizados para calcular o gradiente (multiplicando o erro pelo valor do atributo), e então utilizar esses gradientes para treinar a rede utilizando Gradiente Descendente.\n",
    "\n",
    "**Obs.:** Iremos tratar apenas do caso de Redes com apenas uma Camada Oculta, facilitando as implementações e explicações.\n",
    "\n",
    "### Forward Propagation\n",
    "A ativação de cada camada é calculada exatamente como fazíamos no caso do Singlelayer Perceptron. Para calcularmos a ativação de um neurônio em uma cada $l$ utilizamos os neurônios da camada anterior $l-1$, fazemos:\n",
    "\n",
    "$$\n",
    "    h^{(l)}(\\mathbf{W}^{(l)}) = \\varphi(S_\\text{net}^{(l)}) = \\varphi(W_0^{(l)} + W_1^{(l)} h_1^{(l-1)} + W_2^{(l)} h_2^{(l-1)} + \\cdots + W_n^{(l)} h_n^{(l-1)})\n",
    "$$\n",
    "\n",
    "Onde $S_\\text{net}^{(l)}$ é o **potencial de rede** acumulado na camada anterior em relação ao neurônio que será ativado. Semelhante a anteriormente, podemos calcular facilmente toda a matriz $\\mathbf{h}$ utilizando multiplicação matricial, uma vez que cada elemento é resultado de uma soma ponderada. Dessa forma, realizamos a ativação da primeira à ultima camada, sempre lembrando de acrescentar uma coluna de 1's em cada matriz de ativação calculada.\n",
    "\n",
    "Para o teste abaixo, o restulado deve ser:\n",
    "\n",
    "> Camada de Entrada:  \n",
    ">  [ 1  5  4  3  2]  \n",
    ">  [ 1  1  2  3  4]  \n",
    ">  [ 1 -2  0  1 -1]  \n",
    ">\n",
    "> Camada Oculta:  \n",
    ">  [  1   5.04347408e-07   1.54465265e-01]  \n",
    ">  [  1   2.03426978e-04   3.33480731e-03]  \n",
    ">  [  1   5.52778637e-04   5.74442517e-01]  \n",
    ">\n",
    "> Camada de Saída:  \n",
    ">  [ 0.49441991]  \n",
    ">  [ 0.67598043]  \n",
    ">  [ 0.10748569]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DEFINIÇÃO DA FUNÇÃO DE FORWARD PROPAGATION ##\n",
    "def forward(X, W, probs=True):\n",
    "    # -- SEU CÓDIGO AQUI -- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TESTE DA FORWARD PROPAGATION ##\n",
    "# Observação\n",
    "X = np.array([[ 5, 4, 3,  2],\n",
    "              [ 1, 2, 3,  4],\n",
    "              [-2, 0, 1, -1]])    \n",
    "\n",
    "# Tensor de Pesos\n",
    "W = []                            \n",
    "\n",
    "# 1ª Matriz: C. Entrada -> C. Oculta \n",
    "W.append(np.array([[0.5,-0.7],\n",
    "                   [  1,   2], \n",
    "                   [ -3,  -4],\n",
    "                   [ -4,   3],\n",
    "                   [  2,  -2]]))  \n",
    "\n",
    "# 2ª Matriz: C. Oculta -> C. Saída\n",
    "W.append(np.array([[0.75],\n",
    "                   [  10],\n",
    "                   [  -5]]))      \n",
    "\n",
    "# Cálculo das Ativações\n",
    "A = forward(X,W)\n",
    "\n",
    "# Visualização dos Resultados\n",
    "print(\"# Resultado das Ativações da Rede Neural #\")\n",
    "print(\"Camada de Entrada:\\n\", A[0])\n",
    "print(\"\\nCamada Oculta:\\n\", A[1])\n",
    "print(\"\\nCamada de Saída:\\n\", A[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "A fundamentação teórica que justifica os cálculos do Backpropagation é um pouco complicada, mas o seu algoritmo não é! \n",
    "\n",
    "Primeiramente, sabemos que o erro na última camada é diretamente a diferença entre o que foi calculado por ativação na _Forward Propagation_ e o que nós sabemos pela própria matriz de saídas do conjunto de dados. A partir daí podemos \"retropropagar\" o erro da seguinte forma: o erro em um neurônio da camada $l$ é calculado ao somar cada erro dos neurônios da camada seguinte $l+1$, porém ponderado pelos pesos das sinapses que ligam esses neurônios. Além disso, também multiplicamos o erro pela derivada da ativação deste neurônio, para sabermos a direção que o erro estava mudando.\n",
    "\n",
    "Dessa forma, a fórmula do erro de um neurônio $i$, calculado por Backpropagation, é\n",
    "\n",
    "$$\n",
    "    e_i^{(l)} = \\left( e_1^{(l+1)} W_1^{(l)} + e_2^{(l+1)} W_2^{(l)} + \\cdots + e_n^{(l+1)} W_n^{(l)} \\right) \\cfrac{d \\varphi(S_{i_\\text{net}}^{(l)})}{dW}\n",
    "$$\n",
    "\n",
    "Onde a derivada da ativação é calculada como:\n",
    "\n",
    "$$\n",
    "    \\cfrac{d \\varphi(S_{i_\\text{net}}^{(l)})}{dW} = h_i^{(l)}(1-h_i^{(l)})\n",
    "$$\n",
    "\n",
    "Note, novamente, que as operações não passam de somas ponderadas. Por esse motivo, podemos facilmente calcular todos os erros usando multiplicação matricial.\n",
    "\n",
    "Para o teste abaixo, o restulado deve ser:\n",
    "\n",
    "> Erro na Camada Oculta:  \n",
    "> [ -2.54987879e-06,   3.30158326e-01]  \n",
    "> [ -6.59009122e-04,   5.38469706e-03]  \n",
    "> [  5.93829509e-04,   -1.31378855e-01]  \n",
    "\n",
    "> Erro na Camada de Saída:  \n",
    "> [-0.50558009]  \n",
    "> [-0.32401957]  \n",
    "> [ 0.10748569]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DEFINIÇÃO DA FUNÇÃO DE BACKPROPAGATION ##\n",
    "def backprop(A, y, W):\n",
    "    # -- SEU CÓDIGO AQUI -- #\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TESTE DA BACKPROPAGATION ##\n",
    "# Observação\n",
    "X = np.array([[ 5, 4, 3,  2],\n",
    "              [ 1, 2, 3,  4],\n",
    "              [-2, 0, 1, -1]])    \n",
    "y = np.array([[1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Tensor de Pesos\n",
    "W = []                            \n",
    "\n",
    "# 1ª Matriz: C. Entrada -> C. Oculta \n",
    "W.append(np.array([[0.5,-0.7],\n",
    "                   [  1,   2], \n",
    "                   [ -3,  -4],\n",
    "                   [ -4,   3],\n",
    "                   [  2,  -2]]))  \n",
    "\n",
    "# 2ª Matriz: C. Oculta -> C. Saída\n",
    "W.append(np.array([[0.75],\n",
    "                   [  10],\n",
    "                   [  -5]]))      \n",
    "\n",
    "# Cálculo da Ativação e dos Erros da Rede Neural\n",
    "A = forward(X,W)\n",
    "E = backprop(A,y,W)\n",
    "\n",
    "# Visualização dos Resultados\n",
    "print(\"# Resultado dos Erros da Rede Neural #\")\n",
    "print(\"Erro na Camada Oculta:\\n\", E[0])\n",
    "print(\"\\nErro na Camada de Saída:\\n\", E[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2 - Treinando a Rede Neural Multicamada\n",
    "O Treino de uma Rede Neural multicamada é um processo de otimização exatamente igual ao que realizamos no caso do _Singlelayer Perceptron_.\n",
    "\n",
    "Nesse caso, temos apenas que nos lembrar de que existem duas camadas de pesos que devem ser atualizadas. Todavia, cada uma delas pode ser atualizada individualmente assim como fizemos anteriormente: utilizando o **Gradiente Descendente**. Nesse caso, lembre-se que todas as ativações, assim como os erros, são calculados através de uma função. Essas são as informações que utilizamos para atualizar os pesos. _Abaixo, reescrevemos o algoritmo do Gradiente Descendente com mudança em algumas notações para facilitar o entendimento da implementação_:\n",
    "\n",
    "### Gradiente Descendente\n",
    "\n",
    "Seja uma Rede Neural multicamada com $L$ camadas, onde $A^{(l)}$ é a matriz de ativação em cada camada $l$ e $E^{(l)}$ é a matriz de ativação em cada camada $l$. Nesse caso, $A^{(L)}$ representa o valor de ativação da _Camada de Saída_ e $E^{(L)}$ representa a diferença dessa ativação para os valores reais de $\\mathbf{y}$. O algoritmo do Gradiente Descendente pode ser implementado como:\n",
    "\n",
    "1. Calculamos todas as ativações, $A^{(l)} = h(\\mathbf{W}^{(i)})$, de todos os exemplos utilizando os pesos $\\mathbf{W}^{(i)}$ para essa $i$-ésima época;  \n",
    "\n",
    "2. Utilizamos as ativaçoes, $A^{(l)}$, e os valores reais, $\\mathbf{y}$, para calcular a acurácia total do modelo (utilizando a função _accuracyFunction_) e o _Erro Quadrático Médio_ (MSE), representado pela fórmula:\n",
    "\n",
    "    $$ \n",
    "        J(\\mathbf{w}^{(i)}) = MSE^{(i)} = \\cfrac{1}{m} \\sum_{i=0}^n (A^{(L)} - \\mathbf{y})^2 \n",
    "    $$\n",
    "   \n",
    "   e adicionamos o resultado em um _histórico de erros_.  \n",
    "   **Obs.:** O MSE é a nossa função de Custo Total, comumente representada como $J(\\mathbf{W}^{(i)})$  <br>\n",
    "   \n",
    "3. Realizamos o passo do gradiente para cada camada, atualizando os pesos $\\mathbf{W}^{(i)}$ de uma camada de acordo com a regra:\n",
    "\n",
    "    $$ \n",
    "        \\mathbf{W}^{(i+1)} = \\mathbf{W}^{(i+1)} - \\alpha \\nabla J(\\mathbf{W}^{(i)})\n",
    "    $$\n",
    "    \n",
    "    onde $\\alpha$ é um ajuste de escala arbitrário (geralmente um valor menor que 1) denominado _Taxa de Aprendizagem_ e $\\nabla J(\\mathbf{W}^{(i)})$ representa o vetor gradiente da função de Custo Total, que pode ser calculado matricialmente como:\n",
    "    \n",
    "    $$ \n",
    "        \\nabla J(\\mathbf{W}^{(i)}) = (A^{(l)})^T E^{(l)}\n",
    "    $$  <br>\n",
    "    \n",
    "4. Imprimimos o resultado do treinamento (valores dos pesos, acurácia, erros calculados, etc.) a cada 50 épocas;  <br>\n",
    "\n",
    "5. Checamos se a diferença do erro anterior para o erro atual é menor que a tolerância indicada (indicando possível convergência). Se for verificada a convergência, interrompemos o laço das iterações; <br>\n",
    "\n",
    "6. Caso não houve convergência, atualizamos as variáveis auxiliares e retornamos ao Passo 1. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#    Treinamento do MLP    #\n",
    "###############################\n",
    "def mlp_train(X, y, n_hidden, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=False):\n",
    "    # -- SEU CÓDIGO AQUI -- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 3 - Classificação Binária com MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que possuímos todas as ferramentas devidamente implementadas, podemos utilizar nossa Rede Neural para aprender padrões não-lineares em conjuntos de dados destinados a _Classificação_. Nesse notebook, utilizaremos um conjunto de dados artificial gerado para que possamos visualizar como se comporta essa Rede Neural.\n",
    "\n",
    "Os dados já estão carregados como as variáveis **X_train** e **y_train**. Execute os blocos abaixo para visualizar a Rede Neural em ação. \n",
    "\n",
    "### Execução do Treinamento\n",
    "\n",
    "Primeiramente, executamos o método de treinamento para obtermos o conjunto ótimo de pesos para cada sinapse. É **recomendável** que o aluno \"brinque\" um pouco com os parâmetros do método para entender melhor como os parâmetros influenciam no treinamento. No caso de Redes Neurais multicamadas, o número de neurônios na Camada Oculta muda completamente a complexidade dos modelos que a Rede pode aprender; tente modificar esse parâmetro e notar as diferenças!\n",
    "\n",
    "**Obs.:** O resultado do treinamento também depende da inicialização dos pesos das sinapses, que é aleatória, e por isso talvez seja necessário re-executar o treinamento caso os resultados não sejam bons. (Essa é uma desvantagem do Gradiente Descendente).\n",
    "\n",
    "**Obs.2:** O método deve convergir. Caso contrário, reveja a implementação do Gradiente Descendente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EXECUÇÃO DO TREINAMENTO ##\n",
    "[W, errorHist] = mlp_train(X=X_train, y=y_train, n_hidden=20, alpha=0.1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histórico de Erro do Treinamento\n",
    "\n",
    "O Gradiente Descendente também possui uma característica bem legal de que a função de custo que estamos utilizando, o MSE, sempre decresce a cada iteração do treinamento, em busca do valor mínimo. Se plotarmos o histórico de erro, podemos visualizar como o treinamento se comportou durante os passos dsse método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## VISUALIZAÇÃO DO HISTÓRICO DE ERROS ##\n",
    "plt.figure()\n",
    "plt.plot(errorHist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização da Classificação\n",
    "\n",
    "Agora que temos o conjunto de pesos devidamente treinados, podemos construir uma visualização para exibir como a Rede Neural está classificando os dados em todo o espaço. A função _plotContour_, que já está implementada, recebe um conjunto de pesos já treinados e classifica todos os pontos no plano de visualização. A classe atribuída a cada área corresponde à cor exibida no fundo do plano. A linha preta que divide as áreas é denominada de **Decision Boundary**, e é interpretada como a região do espaço dos exemplos em que podemos notar a divisão das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## VISUALIZAÇÃO DA CLASSIFICAÇÃO ##\n",
    "plt.figure()\n",
    "\n",
    "plt.title(\"Artificial Dataset\"); plt.xlabel(\"$X_1$\"); plt.ylabel(\"$X_2$\")\n",
    "\n",
    "plotContour(X_train, forward, W)\n",
    "\n",
    "a_class = np.where(y_train == 1)\n",
    "b_class = np.where(y_train == 0)\n",
    "\n",
    "plt.scatter(X_train[a_class,0], X_train[a_class,1], marker=\"s\", color=colorpad[0], edgecolor=\"#2A2A2A\", label=\"Class A\")\n",
    "plt.scatter(X_train[b_class,0], X_train[b_class,1], marker=\"o\", color=colorpad[1], edgecolor=\"#2A2A2A\", label=\"Class B\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
