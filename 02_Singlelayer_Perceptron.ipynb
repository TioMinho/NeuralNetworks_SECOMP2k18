{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Neural Networks\n",
    "# 02 - Singlelayer Perceptron\n",
    "\n",
    "<p>\n",
    "    Esse é o nosso primeiro tutorial realmente sobre Redes Neurais! Nesse tutorial, iremos estudar a Rede Neural de camada-única mais simples: a Singlelayer Perceptron. <br>Mais específicamente, iremos aprender como:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>Programar e avaliar funções básicas de Redes Neurais</li>\n",
    "    <li>Desenvolver um algoritmo iterativo para treinar uma Rede Neural, adaptando seus pesos;</li>\n",
    "    <li>Utilizar uma Rede Neural para realizar classificação binária em um dataset;</li>\n",
    "    <li>Utilizar uma Rede Neural para realizar classificação multiclasse em um outro dataset;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas, Variáveis Importantes e Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## BIBLIOTECAS ##\n",
    "import pandas as pd                 # <-- Representação e Manipulação dos Dados\n",
    "import numpy as np                  # <-- Biblioteca para Operações Matriciais e Complexas\n",
    "import matplotlib.pyplot as plt     # <-- Biblioteca para Visualização de Dados\n",
    "\n",
    "## VARIÁVEIS IMPORTANTES ##\n",
    "colorpad = [\"#d11141\", \"#00aedb\", \"#00b159\", \"#f37735\", \"#ffc425\"]\n",
    "colorpadBG = [\"#00aedb50\", \"#d1114150\", \"#00b15950\", \"#f3773550\", \"#ffc42550\"]\n",
    "\n",
    "## FUNÇÕES AUXILIARES ##\n",
    "# Plota um fundo colorido para as visualizações\n",
    "def plotContour(X, model, theta, nClasses=2):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()], theta, False) \n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contour(xx, yy, Z, colors=\"#2A2A2A\", linewidths=2)\n",
    "    plt.contourf(xx, yy, Z, levels=range(-1,nClasses), colors=colorpadBG)\n",
    "\n",
    "    \n",
    "# Essa linha abaixo é apenas para que os \n",
    "# plots sejam gerados na mesma célula do código\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "<p>\n",
    "    Neste tutorial, utilizaremos um dataset artificial, criado apenas para servir como base à explicação dos conceitos relacionados ao treino de Redes Neurais de camada-única. <br>\n",
    "    \n",
    "    O código abaixo já está pronto. Para mais detalhes sobre como resgatar e manipular dados de arquivos, refira-se ao Tutorial 01.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CARREGANDO E PRÉ-PROCESSANDO OS DADOS ##\n",
    "# Carregando os dados a partir do arquivo .csv\n",
    "data = pd.read_csv(\"datasets/data_01.csv\")\n",
    "\n",
    "# Armazenando as dimensões dos dados\n",
    "m = data.shape[0]\n",
    "n = data.shape[1]-1\n",
    "\n",
    "# Separação do Conjunto de Treino e Conjunto de Teste\n",
    "trainingSize = int(0.8 * m)\n",
    "indexes = np.random.randint(0, m, m)\n",
    "trainData = data.iloc[indexes[:trainingSize]]\n",
    "testData = data.iloc[indexes[trainingSize:]]\n",
    "\n",
    "# Obtendo matrizes (formato Numpy) correspondentes\n",
    "X_train = trainData.iloc[:,:-1].values\n",
    "y_train = trainData.iloc[:, -1].values\n",
    "\n",
    "X_test = testData.iloc[:,:-1].values\n",
    "y_test = testData.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## VISUALIZAÇÃO CONDICIONADA COM O MATPLOTLIB E NUMPY ##\n",
    "plt.figure()\n",
    "\n",
    "a_class = np.where(y_train == \"A\")\n",
    "b_class = np.where(y_train == \"B\")\n",
    "\n",
    "plt.title(\"Artificial Dataset\"); plt.xlabel(\"$X_1$\"); plt.ylabel(\"$X_2$\")\n",
    "\n",
    "plt.scatter(X_train[a_class,0], X_train[a_class,1], marker=\"s\", color=colorpad[0], edgecolor=\"#2A2A2A\", label=\"Class A\")\n",
    "plt.scatter(X_train[b_class,0], X_train[b_class,1], marker=\"o\", color=colorpad[1], edgecolor=\"#2A2A2A\", label=\"Class B\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 -  Funções Básicas de Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Função Sigmoid\n",
    "<p>\n",
    "    Como vimos, uma Rede Neural consiste num conjunto de <i>neurônios artificiais</i> interligados por \"sinapses\" que possuem pesos associados. Antes de desenvolvermos os algoritmos que ajustam automaticamente estes pesos, é necessário programar as funções básicas de ativação dos neurônios, e da Rede Neural inteira.<br><br>\n",
    "    \n",
    "    Lembre-se que um neurônio artificial realiza duas computações:\n",
    "    <ul>\n",
    "        <li>Soma ponderada dos sinais de entradas;</li>\n",
    "        <li>Ativação (ou não) do sinal acumulado;</li>\n",
    "    </ul>\n",
    "    \n",
    "    Primeiramente, iremos programar a nossa função de ativação como a sigmoide, representada por: <br> <br>\n",
    "    \n",
    "    $$\n",
    "        \\varphi(z) = \\frac{1}{1 + e^{z}}\n",
    "    $$ \n",
    "    <br>\n",
    "    onde $z$ possa ser também um vetor (e nesse caso a função retorna também um vetor). <br><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição das Funções Sigmoide e Modelo Perceptron\n",
    "def sigmoid(z):\n",
    "    # ...\n",
    "\n",
    "# Teste da Sigmoide\n",
    "dom = np.linspace(-13, 13)\n",
    "sig = sigmoid(dom)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.title(\"Sigmoid function\"); plt.xlabel(\"X\"); plt.ylabel(\"Y\")\n",
    "plt.plot(dom, sig, color=colorpad[3])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Predição\n",
    "\n",
    "<p>\n",
    "    A segunda parte, então, consiste em fazer a função de \"predição\" da SLP, onde os sinais de entrada são ponderados e somados, e então aplicados à função de ativação. Faremos uma condição para que a ativação final possa ter duas formas: a de uma probabilidade (um número real entre 0 e 1) ou seja igual ao número da classe predita (um valor inteiro). <br> <br>\n",
    "    \n",
    "    Temos que: \n",
    "    $$\n",
    "        S_\\text{net} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n y_n\n",
    "    $$\n",
    "    \n",
    "    e\n",
    "    \n",
    "    $$\n",
    "        \\varphi(S_\\text{net}) = \\frac{1}{1 + e^{w_0 + w_1 x_1 + w_2 x_2 + ... + w_n y_n}}\n",
    "    $$\n",
    "    \n",
    "    Lembramos que essa operação pode ser implementada apenas com operações matriciais, reduzindo bastante a complexidade do código. Tente imaginar como seriam essas computações! :)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Função de Predição (Ativação) de uma SLP    \n",
    "def slp_predict(X, theta, probs=True):\n",
    "    # ...\n",
    "\n",
    "# Teste da Ativação\n",
    "theta = np.array([[4.5, 3, -2.5]])\n",
    "X = np.array([[-2.7,  4.1],\n",
    "              [   1,  3.1],\n",
    "              [ 2.3,  4.0],\n",
    "              [   0, -2.4]])\n",
    "\n",
    "print(\"Probability format:\\n\", slp_predict(X, theta))\n",
    "print()\n",
    "print(\"Classification format:\\n\", slp_predict(X, theta, False).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Cálculo de Acurácia\n",
    "Em problemas de Classificação, subtrair o valor previsto do valor real não nos dá uma informação muito precisa (sobre, por exemplo, a gravidade do erro). Para avaliar melhor a qualidade de um Treinamento de Classificação existem diversas medidas. Uma bem comum, e simples, consiste na acurácia: a quantidade de exemplos \"corretamente classificados\". Vamos definir essa porcentagem como sendo:\n",
    "    $$\n",
    "        Acc(\\theta) = 100 \\times (1 - \\frac{1}{m} \\sum (h(\\theta) - y)^{2}) \n",
    "    $$\n",
    "\n",
    "Obs.: a acurácia só será correta caso tanto $h(\\theta)$ quanto $y$ sejam valores binários. Logo, iremos arredondar os resultados de $h(\\theta)$ utilizando a função Predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracyFunction(X, y, theta):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 - Treinando a Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descendente\n",
    "<p>\n",
    "O algoritmo mais popular e simples para treinar automaticamente os parâmetros de uma Rede Neural consiste no Gradient Descendente. Para cada época até a convergência (ou até atingir o limite máximo definido pelo hiperparâmetro) iremos realizar o Treinamento do Single-Layer Perceptron. Os passos serão os seguintes:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>Calculamos o vetor de predição \"predictions\", como resultado da predição do Modelo para os parâmetros daquela época;</li>\n",
    "    <li>Utilizando \"predictions\", calculamos os erros de acordo com o a matriz real \"y\";</li>\n",
    "    <li>Concatenamos o Custo Total do erro calculado no Histórico de Erros;</li>\n",
    "    <li>Realizamos, para cada parâmetro, o \"passo do gradiente\" para estimar os novos valores dos parâmetros;</li>\n",
    "    <li>Imprimimos os resultados do treino a cada 50 épocas;</li>\n",
    "    <li>Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#    SLP Training Function    #\n",
    "###############################\n",
    "def slp_train(X, y, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=False):\n",
    "    # Define the data parameters\n",
    "    # ...    \n",
    "    \n",
    "    # Define the training parameters\n",
    "    # ...\n",
    "    \n",
    "    # Stacks the bias column in the input data\n",
    "    # ...\n",
    "    \n",
    "    # Create the SLP weight matrix\n",
    "    # ...\n",
    "    \n",
    "    # Perform Gradient Descent\n",
    "    while(it <= maxIt):\n",
    "        # Calculate the current predictions and supervisioned error\n",
    "        # ...\n",
    "        \n",
    "        # Calculate the MSE Error and appends to the history\n",
    "        # ...\n",
    "\n",
    "        # Perform the gradient step\n",
    "        # ...\n",
    "        \n",
    "        # Prints the training results after 50 epochs\n",
    "        # ...\n",
    "        \n",
    "        # Prints the training results if converge\n",
    "        # ...\n",
    "            \n",
    "        # Update some training variables\n",
    "        # ...\n",
    "        \n",
    "        it +=1\n",
    "    \n",
    "    # End of the training\n",
    "    return (theta, errorHist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Binary Classification with SLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução de Treinamento\n",
    "<p>\n",
    "Nessa secção avaliamos os resultados das funções de treinamento definidas e implementadas anteriormentes. É altamente recomendável que você \"brinque\" com os parâmetros da função <i>slp_train(.)</i> para entender melhor o funcionamento dos algoritmos de treino\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[theta, errorHist] = slp_train(X_train, y_train, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaçoes dos Resultados do Treino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(errorHist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## VISUALIZAÇÃO DA CLASSIFICAÇÃO ##\n",
    "plt.figure()\n",
    "\n",
    "plt.title(\"Artificial Dataset\"); plt.xlabel(\"$X_1$\"); plt.ylabel(\"$X_2$\")\n",
    "\n",
    "plotContour(X_train, slp_predict, theta)\n",
    "\n",
    "a_class = np.where(y_train == \"A\")\n",
    "b_class = np.where(y_train == \"B\")\n",
    "\n",
    "plt.scatter(X_train[a_class,0], X_train[a_class,1], marker=\"s\", color=colorpad[0], edgecolor=\"#2A2A2A\", label=\"Class A\")\n",
    "plt.scatter(X_train[b_class,0], X_train[b_class,1], marker=\"o\", color=colorpad[1], edgecolor=\"#2A2A2A\", label=\"Class B\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
